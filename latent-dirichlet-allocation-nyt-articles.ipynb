{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"224.068px"},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4876397,"sourceType":"datasetVersion","datasetId":966432}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Latent Dirichlet Allocation - Advanced Machine Learning","metadata":{}},{"cell_type":"markdown","source":"Created a subset of the dataset for news from 2018-2022","metadata":{}},{"cell_type":"markdown","source":" ## Import Libraries and Load Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\nimport gensim\nfrom gensim import corpora, models\nfrom gensim.utils import simple_preprocess\n\n#The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with \n# human language data for applying in statistical natural language processing (NLP). \n\n# It contains text processing libraries for tokenization, parsing, \n# classification, stemming, tagging and semantic reasoning\n\nimport nltk\nnltk.download('punkt')\nnltk.download('omw-1.4')\nfrom nltk.stem import WordNetLemmatizer\n\nfrom nltk.tokenize import RegexpTokenizer\n\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\n\n# Convert a collection of text documents to a matrix of token counts.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Convert a collection of raw documents to a matrix of TF-IDF features.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). \n# Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. \n# This means it can work with sparse matrices efficiently.\nfrom sklearn.decomposition import TruncatedSVD\n\n# The main goal of this python notebook: Latent Dirichlet Allocation\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# T-distributed Stochastic Neighbor Embedding.\nfrom sklearn.manifold import TSNE\n\ntry:\n    import pyLDAvis\n    import pyLDAvis.lda_model  \n    pyLDAvis.enable_notebook()\nexcept:\n    !pip install pyLDAvis\n    import pyLDAvis\n    import pyLDAvis.lda_model  \n    pyLDAvis.enable_notebook()\n\n\nfrom collections import  Counter\n\ntry:\n    from wordcloud import WordCloud\nexcept:\n    !pip install wordcloud\n    from wordcloud import WordCloud\n\n# Stop warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:43.455657Z","start_time":"2023-12-11T15:03:39.604204Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A New York Times dataset (Year 2018 - 2022)\ndf = pd.read_csv(\"data/nyt_clean.csv\")","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:46.775382Z","start_time":"2023-12-11T15:03:43.457665Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()\ndf.head()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:23:50.134114Z","start_time":"2023-12-11T15:23:50.018420Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['section_name'].nunique()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:26:32.472847Z","start_time":"2023-12-11T15:26:32.447703Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## Preprocess and Explore Data","metadata":{}},{"cell_type":"code","source":"\"\"\" We will explore the Abstract and Lead Paragraph column one-by-one \"\"\"","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:46.887518Z","start_time":"2023-12-11T15:03:46.882604Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counting how many headline each section have \nNew_section_type = df.groupby('section_name').count()['headline'].reset_index()\nNew_section_type = New_section_type.sort_values(by='headline', ascending=False)\n# New_section_type\ntop_30_section = New_section_type.head(30)\ntop_30_section_names = top_30_section['section_name'].tolist()\ntop_30 = df[df['section_name'].isin(top_30_section_names)]","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:47.042186Z","start_time":"2023-12-11T15:03:46.889524Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nsection_counts = top_30.groupby(\"section_name\").count()[\"headline\"].reset_index()\n\nsection_counts['color'] = section_counts['headline'].apply(lambda x: 'red' if x > 7000 else 'blue')\n\nfig = px.scatter(section_counts, x=\"section_name\", y=\"headline\", size=\"headline\", title=\"Headline based on each section\",\n                 labels={\"headline\": \"Number of Headlines\", \"section_name\": \"Section Name\"},\n                 size_max=50, color='color')  # Set the color parameter to the new 'color' column\n\nfig.show()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:48.873828Z","start_time":"2023-12-11T15:03:47.043194Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"We will explore the World section for the abstract column and Travel section for lead paragraph column-->\")","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:48.882748Z","start_time":"2023-12-11T15:03:48.876847Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1 = df.loc[df['section_name']=='World']\ndata1.shape","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:48.923862Z","start_time":"2023-12-11T15:03:48.884758Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import textwrap\n\ntext = \"\"\"The exploratory analysis focuses on word frequency, sentence length, average word length analysis, which will help to understand the fundamental characteristics of the text data.\n\nThe chart shows nouns that should be no separated as a word single, such as Trump showing a separate word. The next stage will use the Ngram method to solve the problem and mix the words that have the same meaning into one.\n\nNgrams are defined as contiguous sequences of n words. If the number of words has the same meaning as Donald Trump, it's called Bigram, and if it's more than two words, then it's considered as a trigram.\n\nThe chart above shows the top 20 bigrams in the news headline, which is the most frequent words after removing stop words and lemmatization. Comparing the former one, we can see the words more meaningful and not repeated twice, but there still duplicate words in different deadlines. Using bigrams could improve our data retrieval to be more meaningful, but we still need to improve it.\"\"\"\n\nwrapped_text = textwrap.fill(text, width=80)  # Adjust the width as needed\n\nprint(wrapped_text)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:48.931571Z","start_time":"2023-12-11T15:03:48.925866Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"\"\"Data cleaning is absolutely crucial for generating a useful topic model: as the saying goes, “garbage in, garbage out.” The steps below are common to most natural language processing methods:\n\n - Tokenizing: converting a document to its atomic elements.\n - Stopping: removing meaningless words.\n - Lemmatization: Lemmatization is the process of grouping together different inflected forms of the same word.\"\"\"\n\nparagraphs = text.split('\\n')\n\nwrapped_paragraphs = [textwrap.fill(paragraph, width=80) for paragraph in paragraphs]\n\nwrapped_text = '\\n'.join(wrapped_paragraphs)\n\nprint(wrapped_text)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:48.939414Z","start_time":"2023-12-11T15:03:48.934531Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Punctuation Cleaning, Tokenization and Lemmatization","metadata":{}},{"cell_type":"code","source":"#function to count the number of words in the article\ndef word_count(article):\n  return len(article.split(\" \"))","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:48.943742Z","start_time":"2023-12-11T15:03:48.940481Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting word count of each abstract\ndata1['word_count']=data1['abstract'].apply(word_count) ","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:49.007613Z","start_time":"2023-12-11T15:03:48.944914Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Total number of words present in the whole abstract corpus \nTotal_words1=sum(data1['word_count'])\nTotal_words1","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:49.017739Z","start_time":"2023-12-11T15:03:49.009616Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the sixteenth abstract\ndata1['abstract'].iloc[15]","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:49.026189Z","start_time":"2023-12-11T15:03:49.019744Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing punctuation from the sixteenth abstract\nimport string\ntext_without_punctuation = ''.join([char for char in data1['abstract'].iloc[15] if char not in ('\"', '“', ',' '”', \"'\", \"—\", \"’\", '.') and char not in string.punctuation])\ntext_without_punctuation\n\n#added extra characters in the punctuation as “ != \", similarly for other characters","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:49.033846Z","start_time":"2023-12-11T15:03:49.027193Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizing the sixteenth abstract\n\n# initializing word tokenizer\nword_list  = word_tokenize(text_without_punctuation)\nword_list","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:49.051542Z","start_time":"2023-12-11T15:03:49.034852Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemmatizing the sixteenth abstract\n\nlemma = WordNetLemmatizer()\nword_list = [lemma.lemmatize(word) for word in word_list]\nword_list","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:50.628683Z","start_time":"2023-12-11T15:03:49.053547Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Notice the changes in the words post lemmatizing\"\"\"","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:50.636027Z","start_time":"2023-12-11T15:03:50.631907Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining a combined function for all three steps above\n\ndef lemmatize_word(text):\n    text_without_punctuation = ''.join([char for char in text if char not in ('\"', '“', '”', \"'\", \"—\", \"’\", '.') and char not in string.punctuation])\n    word_list = word_tokenize(text_without_punctuation)\n    lemma = WordNetLemmatizer()\n    word_list = [lemma.lemmatize(word) for word in word_list]\n    return ' '.join(word_list)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:50.641600Z","start_time":"2023-12-11T15:03:50.638032Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying lemmatization on the 'abstract' column which will be called as corpus hereafter\ndata1['abstract']=data1['abstract'].apply(lemmatize_word)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:59.260085Z","start_time":"2023-12-11T15:03:50.643605Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.head()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:59.275745Z","start_time":"2023-12-11T15:03:59.261092Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Top 20 most frequent words in the corpus (abstract)\nTop_20_freq = pd.Series(' '.join(data1['abstract']).split()).value_counts()[:20]\ntop20=pd.DataFrame()\ntop20['Words']= list(Top_20_freq.index)\ntop20['Counts']=list(Top_20_freq.values)\n\nfig, ax = plt.subplots(figsize=(20,8))\nsns.barplot(x = 'Words',y = 'Counts',data = top20,ax=ax).set(title='Top 20 most frequent words in the corpus')","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:03:59.894889Z","start_time":"2023-12-11T15:03:59.277755Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing short length words ( length less than 3)","metadata":{}},{"cell_type":"code","source":"#Removing short length words ( length less than 3)\ndef short_length(text):\n  return (' '.join([wds for wds in text.split() if len(wds)>2]))\n\ndata1['abstract']=data1['abstract'].apply(short_length)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:00.047499Z","start_time":"2023-12-11T15:03:59.896901Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total number of words present in the whole corpus\nTotal_words1 = sum(data1['word_count'])\nTotal_words1","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:00.067727Z","start_time":"2023-12-11T15:04:00.051549Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word count after removal of short length\ndata1['wc_rem_short_len']=data1['abstract'].apply(word_count) \ndisplay(data1.head())","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:00.125561Z","start_time":"2023-12-11T15:04:00.069736Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Notice the word_count and wc_rem_short_len columns in the above dataset\"\"\"","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:00.132551Z","start_time":"2023-12-11T15:04:00.127608Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing Stopwords","metadata":{}},{"cell_type":"code","source":"#Stopwords\n\n# list of stopwords from nltk\nstopwords_nltk = list(stopwords.words('english'))\n\n# # list of stopwords from spacy\n# sp = spacy.load('en_core_web_sm')\n# stopwords_spacy = list(sp.Defaults.stop_words)\n\n# list of stopwords from gensim\nstopwords_gensim = list(gensim.parsing.preprocessing.STOPWORDS)\n\n# unique stopwords from all stopwords\nall_stopwords = []\nall_stopwords.extend(stopwords_nltk)\n# all_stopwords.extend(stopwords_spacy)\nall_stopwords.extend(stopwords_gensim)\n\n# all unique stop words\nall_stopwords = list(set(all_stopwords))\n\ndef stop_words(text):\n  text = [wd.lower() for wd in text.split() if wd.lower() not in all_stopwords]\n  return \" \".join(text)\nlen(all_stopwords)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:00.147932Z","start_time":"2023-12-11T15:04:00.135558Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removal of Stopwords\ndata1['abstract']=data1['abstract'].apply(stop_words)\n# word count after removing the stopwords\ndata1['after_stop']=data1['abstract'].apply(word_count)\ndata1.head() ","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:03.306246Z","start_time":"2023-12-11T15:04:00.154939Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Notice the after_stop column and the reduction of the number of stop words. \"\"\"","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:03.312390Z","start_time":"2023-12-11T15:04:03.307522Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cloud Words plot for displaying most frequent words in the corpus (abstract)\nfreq_words = ','.join(list(data1['abstract'].values))\n\nwordcloud = WordCloud(width=1366, height=768, random_state=21,max_words=100 ,max_font_size=200,background_color='black').generate(freq_words)\nplt.figure(figsize=(12, 7))\nplt.imshow(wordcloud)\nplt.title('Word cloud of most frequent words')\nplt.axis('off')\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:07.817168Z","start_time":"2023-12-11T15:04:03.314462Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing of non-contextual words","metadata":{}},{"cell_type":"code","source":"#removal of non-contextual words\nadd_stopwords= ['said','told','called','use','know','came','based','way','added','including','got','week','people','year','new','mr.']\n#Function to remove additional stopwords \ndef remove_add_stopwords(text):\n  text = [wd.lower() for wd in text.split() if wd.lower() not in add_stopwords]\n  return \" \".join(text)\n#Removing additional stopwords according to use case\ndata1['abstract']=data1['abstract'].apply(remove_add_stopwords)\n# word count after removing the additional stopwords\ndata1['wc_rem_stopword']=data1['abstract'].apply(word_count)\ndata1.head()   ","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:08.126616Z","start_time":"2023-12-11T15:04:07.818173Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Top 20 most frequent words in the corpus After stopword removal\nTop_20_freq = pd.Series(' '.join(data1['abstract']).split()).value_counts()[:20]\ntop20=pd.DataFrame()\ntop20['Words']= list(Top_20_freq.index)\ntop20['Counts']=list(Top_20_freq.values)\n\nfig, ax = plt.subplots(figsize=(20,8))\nsns.barplot(x = 'Words',y = 'Counts',data = top20,ax=ax).set(title='Top 20 most frequent words in the corpus After stopword removal')","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:08.714945Z","start_time":"2023-12-11T15:04:08.128627Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Notice that the top 20 words have more context\"\"\"","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:08.722987Z","start_time":"2023-12-11T15:04:08.718953Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualization of top 20 bi-gram & tri-gram","metadata":{}},{"cell_type":"code","source":"def get_top_n_gram(corpus, ngram, n=20):\n\n    ''' \n    This functions takes entire corpus and returns list of tuple in  pair words and number of its occurence.\n    '''\n    # creates instance of count vectorizer for n-grams\n    vec = CountVectorizer(ngram_range=(ngram,ngram)).fit(corpus)\n\n    # returns sparse matrix of index as documents and columns as Bow as features\n    bag_of_words = vec.transform(corpus)\n\n    # returns 2D array with count of features in corpus\n    sum_words = bag_of_words.sum(axis=0) \n\n    # returns list of tuples with text,counts pair\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n\n    # sort the list of tuples by its number of occurence\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n\n    # returns list of tuples top n words\n    return words_freq[:n]","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:08.736085Z","start_time":"2023-12-11T15:04:08.724530Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get top 20 bigram words using function\ncommon_words = get_top_n_gram(data1['abstract'],ngram=2)\n\n# creates dataframe from list of tuples \ndf1 = pd.DataFrame(common_words, columns = ['Text' , 'count'])\n\n# plot top 20 words\nsns.set(font_scale = 1)\n\nfig, ax = plt.subplots(figsize=(20,8))\nsns.barplot(y= 'Text',x = 'count',data = df1,ax=ax).set(title='Top 20 bigram words')","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:11.903735Z","start_time":"2023-12-11T15:04:08.739092Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get top 20 trigram words using function\ncommon_words = get_top_n_gram(data1['abstract'],ngram=3)\n\n# creates dataframe from list of tuples \ndf2 = pd.DataFrame(common_words, columns = ['Text' , 'count'])\n\n# plot top 20 words\nsns.set(font_scale = 1)\n\nfig, ax = plt.subplots(figsize=(20,8))\nsns.barplot(y= 'Text',x = 'count',data = df2,ax=ax).set(title='Top 20 trigram words')","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:15.006916Z","start_time":"2023-12-11T15:04:11.904782Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Most Frequent words in the corpus show in wordcloud","metadata":{}},{"cell_type":"code","source":"#Most Frequent words in the corpus (abstract) show in wordcloud\nfreq_words = ','.join(list(data1['abstract'].values))\n\nwordcloud = WordCloud(width=1366, height=768, random_state=42,max_words=100 ,max_font_size=200,background_color='black').generate(freq_words)\nplt.figure(figsize=(12, 7))\nplt.imshow(wordcloud)\nplt.title('Most Frequent words in the corpus After stopwords removal',size=20)\nplt.axis('off')\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:19.363554Z","start_time":"2023-12-11T15:04:15.007921Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data reduction \nafter_preprocess=data1['wc_rem_stopword'].sum()\nafter_preprocess\n\ndata_reduce=(after_preprocess/Total_words1)*100\nprint(f\"We have reduced{data_reduce: .02f} percent unnecessary words from our corpus\")","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:19.370318Z","start_time":"2023-12-11T15:04:19.365781Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.shape","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:19.382676Z","start_time":"2023-12-11T15:04:19.371382Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lead Paragraph - Preprocessing","metadata":{}},{"cell_type":"code","source":"data2 = df.loc[df['section_name']=='Travel']\ndata2.shape","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:19.405850Z","start_time":"2023-12-11T15:04:19.383682Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting word count of each lead paragraph\ndata2['word_count']=data2['lead_paragraph'].apply(word_count) ","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:19.419814Z","start_time":"2023-12-11T15:04:19.406854Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Total number of words present in the whole lead paragraph corpus \nTotal_words=sum(data2['word_count'])\nTotal_words","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:19.427941Z","start_time":"2023-12-11T15:04:19.422826Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing punctuations \n\nimport re\ndata2['lead_paragraph'] = data2['lead_paragraph'].map(lambda x: re.sub('[,\\.!?—’]', '', x)) #removing punctuations\ndata2['lead_paragraph'] = data2['lead_paragraph'].map(lambda x: x.lower()) # our dataset is already lowercase, this is an additional step here\n\nimport gensim\nfrom gensim.utils import simple_preprocess\n\ndef preprocess_text(text):\n    return gensim.utils.simple_preprocess(str(text), deacc=True)  # deacc=True removes punctuations\n\n# gensim.utils.simple_preprocess lowercases, tokenizes, de-accents (optional). – the output are final tokens = unicode strings, that won’t be processed any further.\n\ndata2['lead_paragraph'] = data2['lead_paragraph'].apply(preprocess_text)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:19.783126Z","start_time":"2023-12-11T15:04:19.428949Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2['lead_paragraph'].iloc[0]","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:19.790784Z","start_time":"2023-12-11T15:04:19.784131Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Top 20 most frequent words in the corpus (lead_paragraph)\n\nTop_20_freq = pd.Series(word for sublist in data2['lead_paragraph'] for word in sublist).value_counts()[:20]\ntop20=pd.DataFrame()\ntop20['Words']= list(Top_20_freq.index)\ntop20['Counts']=list(Top_20_freq.values)\n\nfig, ax = plt.subplots(figsize=(20,8))\nsns.barplot(x = 'Words',y = 'Counts',data = top20,ax=ax).set(title='Top 20 most frequent words in the corpus')","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:20.488052Z","start_time":"2023-12-11T15:04:19.791841Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same for lead_paragraph using another method\n# Load the regular expression library\nimport re\n\n# Remove punctuation\ndata1['lead_paragraph'] = data1['lead_paragraph'].map(lambda x: re.sub('[,\\.!?]', '', x))\n\n# Convert the titles to lowercase\ndata1['lead_paragraph'] = data1['lead_paragraph'].map(lambda x: x.lower())\n\n# Print out the first rows of papers\ndata1['lead_paragraph'].head()\n\n\nimport gensim\nfrom gensim.utils import simple_preprocess\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata = data1.lead_paragraph.values.tolist()\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1][0][:30])\n\n\n# Build the bigram and trigram models\nbigram = gensim.models.Phrases( data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:31.124838Z","start_time":"2023-12-11T15:04:20.491058Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing stop_words from lead_paragraph\n\n# NLTK Stop words\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n\n# Define functions for stopwords, bigrams, trigrams and lemmatization\n\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:04:31.134999Z","start_time":"2023-12-11T15:04:31.126188Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    !python -m spacy download en_core_web_sm\n    import spacy\nexcept Exception as e:\n    print(f\"An error occurred during spaCy setup: {e}\")\n\n# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\nnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:1][0][:30])","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:06:48.857006Z","start_time":"2023-12-11T15:04:31.136586Z"},"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Most Frequent words in the corpus (lead_paragraph) show in wordcloud\nfreq_words = ' '.join([' '.join(doc) for doc in data_lemmatized])\n\nwordcloud = WordCloud(width=1366, height=768, random_state=42,max_words=100 ,max_font_size=200,background_color='black').generate(freq_words)\nplt.figure(figsize=(12, 7))\nplt.imshow(wordcloud)\nplt.title('Most Frequent words in the corpus After stopwords removal',size=20)\nplt.axis('off')\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:06:53.559797Z","start_time":"2023-12-11T15:06:48.859481Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removal of non-contextual words from lead_paragraph corpus\nadd_stopwords= ['say','make','many','take','trip','time','well','day','travel','traveler','know','week','people','year','way','still', 'time']\n#Function to remove additional stopwords \ndef remove_add_stopwords(words):\n    words = [wd.lower() for wd in words if wd.lower() not in add_stopwords]\n    return words\n# Applying the function to each sublist in data_lemmatized\ndata_lemmatized = [remove_add_stopwords(doc) for doc in data_lemmatized]","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:06:53.849479Z","start_time":"2023-12-11T15:06:53.561805Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Most Frequent words in the corpus (lead_paragraph) show in wordcloud\nfreq_words = ' '.join([' '.join(doc) for doc in data_lemmatized])\n\nwordcloud = WordCloud(width=1366, height=768, random_state=42,max_words=100 ,max_font_size=200,background_color='black').generate(freq_words)\nplt.figure(figsize=(12, 7))\nplt.imshow(wordcloud)\nplt.title('Most Frequent words in the corpus After stopwords removal',size=20)\nplt.axis('off')\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:06:59.729939Z","start_time":"2023-12-11T15:06:53.851484Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling - Latent Dirichlet Allocation","metadata":{}},{"cell_type":"markdown","source":"### Term Frequency Inverse Document Frequency (TF-IDF)","metadata":{}},{"cell_type":"code","source":"#Tf-Idf vectoriser\n\n# min_df = 0.03 means that the words (terms) should appear in at least 3% \n# of the documents to be considered in the TF-IDF matrix. words that appear in \n# less than 3% of the documents will be ignored in the TF-IDF matrix.\n\nvectorizer = TfidfVectorizer(max_df=0.95, min_df=0.03) \ndocument_term_matrix = vectorizer.fit_transform(data1['abstract'])\n#Shape of document term matrix\ndocument_term_matrix.shape","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:07:00.230882Z","start_time":"2023-12-11T15:06:59.732450Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(document_term_matrix) ","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:07:00.237649Z","start_time":"2023-12-11T15:07:00.232891Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_top_words(model, feature_names, n_top_words, title):\n    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[-n_top_words:]\n        top_features = feature_names[top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:07:00.247405Z","start_time":"2023-12-11T15:07:00.239657Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time\nlda_model = LatentDirichletAllocation(n_components=10,\n    max_iter=5,\n    learning_method=\"online\",\n    learning_offset=50.0,\n    random_state=42)\n\nt0 = time()\n\nlda_model.fit(document_term_matrix)\n\nprint(\"done in %0.3fs.\" % (time() - t0))\n\ntf_feature_names = vectorizer.get_feature_names_out()\n\nplot_top_words(lda_model, tf_feature_names, 20, \"Topics in LDA model\")","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:07:11.703250Z","start_time":"2023-12-11T15:07:00.250420Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameters tuning using Grid Search\nfrom sklearn.model_selection import GridSearchCV\ngrid_params = {'n_components' : list(range(5,10))}\n\n# LDA model\nlda_model = GridSearchCV(LatentDirichletAllocation(learning_method ='online', random_state=42),param_grid=grid_params)\nlda_model.fit(document_term_matrix)\n\n# Best LDA model\nbest_lda_model = lda_model.best_estimator_\n\nprint(\"Best LDA model's params\" , lda_model.best_params_)\nprint(\"Best log likelihood Score for the LDA model\",lda_model.best_score_)\nprint(\"LDA model Perplexity on train data\", best_lda_model.perplexity(document_term_matrix))","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:13:38.433658Z","start_time":"2023-12-11T15:07:11.705258Z"},"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LDA model fitting for best parameter\nlda= LatentDirichletAllocation(n_components=5,learning_method ='online',max_iter=10,random_state=42, n_jobs=-1)\nlda.fit(document_term_matrix)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:14:37.546928Z","start_time":"2023-12-11T15:13:38.434663Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LDA model\ntop_lda=lda.fit_transform(document_term_matrix)\n\nprint(top_lda.shape)\nprint(top_lda)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:15:38.366165Z","start_time":"2023-12-11T15:14:37.546928Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# composition of doc 0 for eg\nprint(\"Document 0: \")\nfor i,topic in enumerate(top_lda[0]):\n  print(\"Topic \",i,\": \",topic*100,\"%\")","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:15:38.376848Z","start_time":"2023-12-11T15:15:38.369399Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\"\"The first document is more belonging to the Topic 1.\"\"\")","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:15:38.386869Z","start_time":"2023-12-11T15:15:38.376848Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lda for lead_paragraph\n\nimport gensim.corpora as corpora\n\n# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1][0][:30])","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:15:40.253276Z","start_time":"2023-12-11T15:15:38.386869Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build LDA model\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=10, \n                                       random_state=42,\n                                       chunksize=100,\n                                       passes=10,\n                                       per_word_topics=True)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:16:35.261973Z","start_time":"2023-12-11T15:15:40.254148Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import CoherenceModel\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('Coherence Score: ', coherence_lda)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:16:45.691116Z","start_time":"2023-12-11T15:16:35.261973Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# supporting function\ndef compute_coherence_values(corpus, dictionary, k, a, b):\n    \n    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                           id2word=dictionary,\n                                           num_topics=k, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha=a,\n                                           eta=b)\n    \n    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n    \n    return coherence_model_lda.get_coherence()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:16:45.700191Z","start_time":"2023-12-11T15:16:45.694431Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NB: The code below works but need long time to run, since we use large dataset. So we note it in case stop kernal by run automatically.","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import tqdm\n\n# grid = {}\n# grid['Validation_Set'] = {}\n\n# # Topics range\n# min_topics = 2\n# max_topics = 11\n# step_size = 1\n# topics_range = range(min_topics, max_topics, step_size)\n\n# # Alpha parameter\n# alpha = list(np.arange(0.01, 1, 0.3))\n# alpha.append('symmetric')\n# alpha.append('asymmetric')\n\n# # Beta parameter\n# beta = list(np.arange(0.01, 1, 0.3))\n# beta.append('symmetric')\n\n# # Validation sets\n# num_of_docs = len(corpus)\n# corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n#                corpus]\n\n# corpus_title = ['75% Corpus', '100% Corpus']\n\n# model_results = {'Validation_Set': [],\n#                  'Topics': [],\n#                  'Alpha': [],\n#                  'Beta': [],\n#                  'Coherence': []\n#                 }\n\n# # Can take a long time to run\n# if 1 == 1:\n#     pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n    \n#     # iterate through validation corpuses\n#     for i in range(len(corpus_sets)):\n#         # iterate through number of topics\n#         for k in topics_range:\n#             # iterate through alpha values\n#             for a in alpha:\n#                 # iterare through beta values\n#                 for b in beta:\n#                     # get the coherence score for the given parameters\n#                     cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n#                                                   k=k, a=a, b=b)\n#                     # Save the model results\n#                     model_results['Validation_Set'].append(corpus_title[i])\n#                     model_results['Topics'].append(k)\n#                     model_results['Alpha'].append(a)\n#                     model_results['Beta'].append(b)\n#                     model_results['Coherence'].append(cv)\n                    \n#                     pbar.update(1)\n#     pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)\n#     pbar.close()    ","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:16:45.707852Z","start_time":"2023-12-11T15:16:45.702407Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_topics = 8\n\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=num_topics, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha=0.01,\n                                           eta=0.9)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:17:49.086991Z","start_time":"2023-12-11T15:16:45.710439Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pyLDAvis.gensim_models as gensimvis\n# import pickle \n# import pyLDAvis\n# import os \n# # Visualize the topics\n# pyLDAvis.enable_notebook()\n\n# LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))\n\n# # # this is a bit time consuming - make the if statement True\n# # # if you want to execute visualization prep yourself\n# if 1 == 1:\n#     LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n#     with open(LDAvis_data_filepath, 'wb') as f:\n#         pickle.dump(LDAvis_prepared, f)\n\n# # load the pre-prepared pyLDAvis data from disk\n# with open(LDAvis_data_filepath, 'rb') as f:\n#     LDAvis_prepared = pickle.load(f)\n\n# pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')\n\n# LDAvis_prepared","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:17:49.094881Z","start_time":"2023-12-11T15:17:49.086991Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyLDAvis.lda_model\nlda_panel = pyLDAvis.lda_model.prepare(best_lda_model, document_term_matrix,vectorizer,mds='tsne')\nlda_panel","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:17:52.340632Z","start_time":"2023-12-11T15:17:49.096495Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# most important words for each topic\nvocab = vectorizer.get_feature_names_out()","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:17:52.348414Z","start_time":"2023-12-11T15:17:52.345254Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\n# Generate a word cloud image for given topic\ndef word_cloud_lda(index):\n  imp_words_topic=\"\"\n  comp=lda.components_[index]\n  vocab_comp = zip(vocab, comp)\n  sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:50]\n  for word in sorted_words:\n    imp_words_topic=imp_words_topic+\" \"+word[0]\n\n  wordcloud = WordCloud(width=600, height=400,max_font_size=100).generate(imp_words_topic)\n  plt.figure( figsize=(5,5))\n  plt.imshow(wordcloud)\n  plt.axis(\"off\")\n  plt.tight_layout()\n  plt.show()\n#Word Cloud for each topic\nprint(\"\"\"Topic 0\"\"\")\nword_cloud_lda(0)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:17:52.789151Z","start_time":"2023-12-11T15:17:52.352436Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\"\"Topic 1\"\"\")\nword_cloud_lda(1)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:17:53.195382Z","start_time":"2023-12-11T15:17:52.792202Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\"\"Topic 2\"\"\")\nword_cloud_lda(2)","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:17:53.633267Z","start_time":"2023-12-11T15:17:53.197857Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\"\"Topic 3\"\"\")\nword_cloud_lda(3) ","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:17:54.042584Z","start_time":"2023-12-11T15:17:53.636328Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\"\"Topic 4\"\"\")\nword_cloud_lda(4) ","metadata":{"ExecuteTime":{"end_time":"2023-12-11T15:17:54.428352Z","start_time":"2023-12-11T15:17:54.044963Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 5 different topic present in the aricles.\n\nLDA and LSA are implemented to find the hidden topics.\n\nLDA performs well & shows 5 different clusters present in the Corpus.","metadata":{}}]}